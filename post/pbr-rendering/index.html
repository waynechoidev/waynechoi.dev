<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/2e2a7597edf4d608.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/602feae4669a507e.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-c25d000de3a6a081.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-33f88dcb8cab817d.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-c5427e31248623a0.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-998ad79e34130139.js" async="" crossorigin=""></script><script src="/_next/static/chunks/326-6784f49434136418.js" async=""></script><script src="/_next/static/chunks/app/layout-a7d9660fe841e21f.js" async=""></script><script src="/_next/static/chunks/691-d72542b099e0bc41.js" async=""></script><script src="/_next/static/chunks/app/post/%5Bslug%5D/page-e90f3bafd107b713.js" async=""></script><title>WayneChoi.Dev</title><meta name="description" content="Wayne Choi blog"/><link rel="icon" href="/favicon.ico"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="mx-auto my-0"><nav class="max-w-default mx-auto px-5 py-3"><div class="flex justify-end"><a href="/about/"><span class="hover:font-bold pl-3 text-lg font-normal">About</span></a><a href="/posts/"><span class="hover:font-bold pl-3 text-lg font-normal">Posts</span></a></div></nav><header><div class="h-32 relative" id="sky"><canvas class="w-full h-full"></canvas><div class="absolute top-0 inset-x-auto w-full "><div class="w-full max-w-default mx-auto"><h1 class="text-4xl font-semibold p-5 pt-0"><a class="text-blue-800" href="/">WayneChoi.dev</a></h1></div></div></div></header><div class="max-w-default mx-auto p-5"><div><a class="text-gray-600 text-md pl-1 hover:font-bold" href="/posts/graphics/">GRAPHICS</a><h1>PBR Rendering</h1><p class="text-gray-400 text-sm m-0 mt-1">1 April, 2024</p><div class="mt-1"><div class="whitespace-nowrap overflow-x-auto pb-3"><a class="mr-2 text-gray-500 hover:font-bold" href="/tag/opengl/">#opengl</a><a class="mr-2 text-gray-500 hover:font-bold" href="/tag/pbr/">#pbr</a><a class="mr-2 text-gray-500 hover:font-bold" href="/tag/glsl/">#glsl</a></div></div><div class="mt-8"><div class="flex flex-col justify-center"></div></div><div><div></div></div></div></div><footer class="h-40 flex justify-center items-end"><p class="text-gray-500 text-sm">(C) 2021. Wayne Choi. All rights reserved.</p></footer><script src="/_next/static/chunks/webpack-c25d000de3a6a081.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/2e2a7597edf4d608.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/602feae4669a507e.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"4:I[3728,[],\"\"]\n6:I[9928,[],\"\"]\n7:I[9194,[\"326\",\"static/chunks/326-6784f49434136418.js\",\"185\",\"static/chunks/app/layout-a7d9660fe841e21f.js\"],\"\"]\n8:I[5629,[\"326\",\"static/chunks/326-6784f49434136418.js\",\"185\",\"static/chunks/app/layout-a7d9660fe841e21f.js\"],\"\"]\n9:I[6954,[],\"\"]\na:I[7264,[],\"\"]\nc:I[8326,[\"326\",\"static/chunks/326-6784f49434136418.js\",\"691\",\"static/chunks/691-d72542b099e0bc41.js\",\"605\",\"static/chunks/app/post/%5Bslug%5D/page-e90f3bafd107b713.js\"],\"\"]\nd:I[7708,[\"326\",\"static/chunks/326-6784f494341"])</script><script>self.__next_f.push([1,"36418.js\",\"691\",\"static/chunks/691-d72542b099e0bc41.js\",\"605\",\"static/chunks/app/post/%5Bslug%5D/page-e90f3bafd107b713.js\"],\"\"]\nf:I[5631,[\"326\",\"static/chunks/326-6784f49434136418.js\",\"691\",\"static/chunks/691-d72542b099e0bc41.js\",\"605\",\"static/chunks/app/post/%5Bslug%5D/page-e90f3bafd107b713.js\"],\"\"]\ne:T713b,"])</script><script>self.__next_f.push([1,"\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XrfiBfOx1LE?si=N0abz-D93HKovMzM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- [WebGL Sample](https://waynechoidev.github.io/web-pbr/)\n- [WebGPU Sample](https://waynechoidev.github.io/webgpu-pbr/)\n\nI've implemented PBR rendering using OpenGL, and WebGL. Primarily, this project was developed with reference to the content available at [Learn OpenGL](https://learnopengl.com/). The ideas presented on that website served as inspiration for this implementation. Additionally, the code from [IBL Baker](https://www.derkreature.com/iblbaker) was also consulted for further enhancements. I'll focus more on implementation rather than delving into mathematical and physical theories on this post.\n\nThe following code snippets are intended to supplement the concepts explained as needed and may not represent exact implementations. The complete implementation can be found in the repository link below.\n\n- [OpenGL App Repo](https://github.com/waynechoidev/pbr)\n- [IBL Maker Repo](https://github.com/waynechoidev/ibl-maker)\n- [WebGL App Repo](https://github.com/waynechoidev/web-pbr)\n\n## 1. PBR Textures\n\nPBR (Physically Based Rendering) textures are used to describe the physical properties of materials. Typically, a combination of various texture maps is used to compose a single PBR material. There are several types of textures commonly used, primarily including:\n\n- Albedo Map\n\n  It represents the base color of the surface, indicating the material's actual color.\n\n- Normal Map\n\n  It manipulates the surface's normal vectors to add three-dimensional detail, simulating irregular shapes or textures.\n\n- Metallic Map\n\n  It indicates whether the surface is metallic or non-metallic. It ranges from black (non-metallic) to white (metallic).\n\n- Roughness Map\n\n  It represents the roughness or smoothness of the surface. It ranges from black (smooth) to white (rough).\n\n- Ambient Occlusion Map (AO Map)\n\n  It simulates indirect lighting by considering occlusion of the surface. Darker areas in the texture typically represent surface occlusion or concavities.\n\n- Height Map\n\n  It represents the three-dimensional detail of a surface. It is mainly employed to provide finer details on the surface of objects. Although not covered in [Learn OpenGL](https://learnopengl.com/) (presumably because it is unrelated to lighting), it is included in most PBR textures.\n\nTo use these textures, a normal vector is required. The normal vector indicates the direction of the surface and plays a crucial role in determining the surface's lighting reflection properties based on the lighting and viewpoint between the light source and observer.\n\nNormal vectors are primarily represented through normal maps, with each pixel of the texture representing the local direction of the surface. This information is used in lighting models considering the directions of the light source and observer. For instance, normal vectors provide the angle between the surface and light, allowing for the simulation of highlights and shadows.\n\nNormal maps are commonly used in tangent space. Tangent space defines the local coordinate space of the surface, with basis vectors representing the normal, tangent, and binormal at each point on the surface. Normal maps are stored in this tangent space, facilitating the easy transformation of normals from texture space.\n\nTherefore, to use a normal map, tangent vectors and normal vectors associated with the surface's local coordinate space are required. This information enables the mapping of normals stored in the normal map to the actual normals of the surface.\n\nWhile the LearnOpenGL example attempted to calculate tangent vectors using gradients of WorldPos and TexCoords, this method is an inaccurate estimation. Typically, tangent vectors are provided along with the model, so I passed tangent vectors from the CPU to the vertices.\n\n```glsl\n// the code used in [Learn OpenGL](https://learnopengl.com/) to calculate tangents\n\nvec3 Q1  = dFdx(WorldPos);\nvec3 Q2  = dFdy(WorldPos);\nvec2 st1 = dFdx(TexCoords);\nvec2 st2 = dFdy(TexCoords);\n\nvec3 T  = normalize(Q1*st2.t - Q2*st1.t);\n```\n\nThe normal and tangent vectors calculated in the vertex shader exist in the local coordinate space of the model. When passing these vectors to the fragment shader, they need to be transformed into world coordinates. To achieve this, the vectors should be transformed using the model transformation matrix to convert them into world coordinates.\n\nTypically, the transformation of the tangent vector involves only the model transformation matrix. However, for the normal vector, relying solely on the model transformation may not yield correct results when the normal is defined in a non-standard texture coordinate space. This is because the normal vector represents direction, and transformation matrices only preserve length and rotation, not direction.\n\nTherefore, for the normal vector, it's necessary to use the transpose and inverse of the model transformation matrix for the correct transformation into world coordinates. This ensures that the normal vector is accurately transformed into world coordinates. The reason for using transpose and inverse is to ensure preservation of direction during the transformation, as these operations are used to perform transformations that preserve direction. You can find more detailed explanations at this [link](https://paroj.github.io/gltut/Illumination/Tut09%20Normal%20Transformation.html).\n\n```glsl\n// the code for calculating the normal world and tangent world vectors in the vertex shader\n\nvec3 normalWorld = normalize(mat3(transpose(inverse(model))) * norm);\nvec3 tangentWorld = normalize(model * vec4(tangent, 1.0)).xyz;\n```\n\nFirst, we apply the height map in the Vertex shader. This texture directly manipulates texture coordinates to implement the three-dimensional shape of the surface, rather than affecting lighting or shadows.\n\n```glsl\nuniform sampler2D heightTex;\n\nfloat height = texture(heightTex, tex).r;\nheight = height * 2.0 - 1.0; // 0~1 -\u003e -1~1\nvec3 newPos = position + vec3(normalWorld * height);\n```\n\nAll remaining textures are related to lighting, and we'll cover them from the next chapter onward.\n\n---\n\n## 2. Direct Lighting\n\nDirect Lighting is one of the crucial concepts in Physically Based Rendering (PBR). It deals with scenarios where light directly reflects off an object's surface. In other words, it models the process where light travels in a straight line from the light source to the object, determining aspects like the object's color and shadows. Typically, the Bidirectional Reflectance Distribution Function (BRDF) is used for these calculations. Here, we'll implement it using the widely adopted Cook-Torrance model. This model considers the microstructure of the object's surface and the angle of incidence of light to provide realistic lighting effects. In this explanation, rather than delving into the microfacet theory that deals with fine surface structures, we'll focus on implementing the function. You can find more detailed theoretical information on this topic in this [link](https://blog.selfshadow.com/publications/s2013-shading-course/hoffman/s2013_pbs_physics_math_slides.pdf).\n\n\u003e #### Microfacet BRDF\n\u003e\n\u003e f(l,v) = F(l, h) G(l, v, h) D(h) / 4(n · l)(n · v)\n\nThe equation represents the general form of the BRDF (Bidirectional Reflectance Distribution Function). I'll provide explanations and implementations for each component.\n\n### F (Fresnel Reflectance)\n\n\u003e F(Cspec, l, h) = Cspec + (1 - Cspec)(1 - (l · h)) ^ 5\n\nFresnel Reflectance represents the optical properties when light reaches and reflects off the surface.\n\n```glsl\nuniform sampler2D albedoMap;\nuniform sampler2D metallicMap;\n\n// Apply gamma correction to the sampled albedo texture to convert it from sRGB space to linear space\nvec3 albedo = pow(texture(albedoMap, TexCoord).rgb, vec3(2.2));\nfloat metallic  = texture(metallicMap, TexCoord).r;\n\nvec3 fresnelSchlick(float cosTheta, vec3 F0)\n{\n    return F0 + (1.0 - F0) * pow(clamp(1.0 - cosTheta, 0.0, 1.0), 5.0);\n}\n\n// Initialize the surface reflectance coefficient F0 with a default value\nvec3 F0 = vec3(0.04);\n\n// Linearly interpolate between the surface reflectance coefficient F0 and the albedo (surface color) based on metallic value\nF0 = mix(F0, albedo, metallic);\n\nvec3 F = fresnelSchlick(max(dot(H, V), 0.0), F0);\n```\n\n### G (Geometry Factor)\n\n\u003e GSchlickGGX(n, v, k) = n ⋅ v ( n ⋅ v) (1 − k) + k\n\u003e\n\u003e G(l, v, h) = Gs1(l, h) Gs2(v, h)\n\nGeometry Factor indicates the portions occluded between the light and the observer, modeling effects such as occluded shadows. it takes a material's roughness parameter as input with rougher surfaces having a higher probability of overshadowing microfacets. The geometry function we will use is a combination of the GGX and Schlick-Beckmann approximation known as Schlick-GGX.\n\n```glsl\nuniform sampler2D roughnessMap;\nuniform sampler2D normalMap;\n\nuniform vec3 camPos;\n\nfloat roughness = texture(roughnessMap, TexCoord).r;\n\nvec3 N = normalize(normalWorld);\n// Adjust the tangent vector to ensure it is perpendicular to the surface\n// by removing the component parallel to the normal vector.\nvec3 T = normalize(tangentWorld - dot(tangentWorld, N) * N);\nvec3 B = cross(N, T);\n\n// Create a TBN matrix containing the tangent, binormal, and normal vectors.\nmat3 TBN = mat3(T, B, N);\n\n// Sample the normal map from the texture and map it to the [-1, 1] range to update the normal vector.\nN = normalize(TBN * (texture(normalMap, TexCoord).xyz * 2.0 - 1.0));\n\n// Compute the view vector.\nvec3 V = normalize(camPos - posWorld);\n\n// Compute the light direction vector\nvec3 L = normalize(lightPos - posWorld);\n\nfloat GeometrySchlickGGX(float NdotV, float roughness)\n{\n    float r = (roughness + 1.0);\n    float k = (r * r) / 8.0;\n\n    float nom   = NdotV;\n    float denom = NdotV * (1.0 - k) + k;\n\n    return nom / denom;\n}\n\nfloat GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)\n{\n    float NdotV = max(dot(N, V), 0.0);\n    float NdotL = max(dot(N, L), 0.0);\n    float ggx2 = GeometrySchlickGGX(NdotV, roughness);\n    float ggx1 = GeometrySchlickGGX(NdotL, roughness);\n\n    return ggx1 * ggx2;\n}\n\nfloat G = GeometrySmith(N, V, L, roughness);\n```\n\n### D (Normal Distribution Function)\n\n\u003e NDF(n, h, α) = α ^ 2 / π((n ⋅ h) ^ 2 (α ^ 2 − 1) + 1) ^ 2\n\nNormal Distribution Function denotes the reflection direction due to the microstructure of the surface, considering surface roughness or fine surface characteristics. The NDF function we'll be using is known as the Trowbridge-Reitz GGX.\n\n```glsl\nfloat DistributionGGX(vec3 N, vec3 H, float roughness)\n{\n    float a = roughness*roughness;\n    float a2 = a*a;\n    float NdotH = max(dot(N, H), 0.0);\n    float NdotH2 = NdotH*NdotH;\n\n    float nom   = a2;\n    float denom = (NdotH2 * (a2 - 1.0) + 1.0);\n    denom = PI * denom * denom;\n\n    return nom / denom;\n}\n\nfloat NDF = DistributionGGX(N, H, roughness);\n```\n\n### BRDF\n\n```glsl\nvec3 numerator = NDF * G * F;\n// + 0.0001 to prevent divide by zero\nfloat denominator = 4.0 * max(dot(N, V), 0.0) * max(dot(N, L), 0.0) + 0.0001;\n\nvec3 BRDF = numerator / denominator;\n```\n\nNow we can finally calculate this direct light's contribution to the reflectance equation.\n\nIn the [Learn OpenGL](https://learnopengl.com/) example, they used point lights coming from four directions, but I only used one point light.\n\n```glsl\n// Calculate the specular reflection coefficient.\nvec3 kS = F;\n\n// Calculate the diffuse reflection coefficient.\nvec3 kD = vec3(1.0) - kS;\nkD *= 1.0 - metallic;\n\n// Scale the light intensity by the cosine of the angle between the normal vector and the light direction.\nfloat NdotL = max(dot(N, L), 0.0);\n\n// Calculate the direct lighting contribution, combining diffuse and specular reflections.\nvec3 directLight = (kD * albedo / PI + BRDF) * radiance * NdotL;\n```\n\n---\n\n## 3. Environment Lighting\n\nEnvironment Lighting is primarily implemented using the IBL (Image-Based Lighting) technique. This method involves representing the surrounding environment with appropriate maps and using these maps to calculate lighting for objects. To implement Environment Lighting for PBR (Physically Based Rendering), three main textures are required: the Irradiance Map, Specular Map, and BRDF Texture.\n\n### Irradiance Map\n\nIrradiance Map is used to model the lighting of the surrounding environment. It represents the incoming radiance at a specific point, typically captured and generated around the camera's surroundings. Irradiance Map is utilized to simulate indirect lighting on the surface of objects, thus determining their color and brightness.\n\nIn [Learn OpenGL](https://learnopengl.com/), the environment map is sampled using point sampling in a spherical coordinate system. As the desired results weren't achieved with this method, I opted for importance sampling to sample the Irradiance Map from the environment map. This approach involves sampling the more important areas of the environment map more frequently to obtain a more accurate Irradiance value. To implement this, I referred to the code of an open-source project called [IBL Baker](https://www.derkreature.com/iblbaker) extensively.\n\n```glsl\n#version 420 core\n\nout vec4 FragColor;\n\nin vec3 WorldPos;\n\nuniform samplerCube envCubemap;\n\nconst float PI = 3.14159265359;\n\nconst int ConvolutionSamplesOffset = 1;\nconst int ConvolutionSampleCount = 256;\nconst int ConvolutionMaxSamples = 512;\nconst float ConvolutionRoughness = 0.2;\nconst float ConvolutionMip = 3.0;\nconst float EnvironmentScale = 3.0;\nconst float IblMaxValue[3] = { 102.0, 85.0, 79.0 };\nconst vec4 IBLCorrection = vec4(1.0);\n\n\nvec3 rescaleHDR(vec3 hdrPixel) {\n    hdrPixel = hdrPixel / (hdrPixel + vec3(1.0));\n\n    if (hdrPixel.x \u003c 0.0)\n        hdrPixel.x = 0.0;\n    if (hdrPixel.y \u003c 0.0)\n        hdrPixel.y = 0.0;\n    if (hdrPixel.z \u003c 0.0)\n        hdrPixel.z = 0.0;\n\n    float intensity = dot(hdrPixel, vec3(0.299, 0.587, 0.114));\n\n    // Saturation adjustment\n    hdrPixel = mix(intensity.xxx, hdrPixel, IBLCorrection.y);\n\n    // Hue adjustment\n    vec3 root3 = vec3(0.57735, 0.57735, 0.57735);\n    float half_angle = 0.5 * radians(IBLCorrection.z);\n    vec4 rot_quat = vec4(root3 * sin(half_angle), cos(half_angle));\n    mat3 rot_Matrix = mat3(\n        vec3(1.0 - 2.0 * (rot_quat.y * rot_quat.y + rot_quat.z * rot_quat.z), 2.0 * (rot_quat.x * rot_quat.y - rot_quat.w * rot_quat.z), 2.0 * (rot_quat.x * rot_quat.z + rot_quat.w * rot_quat.y)),\n        vec3(2.0 * (rot_quat.x * rot_quat.y + rot_quat.w * rot_quat.z), 1.0 - 2.0 * (rot_quat.x * rot_quat.x + rot_quat.z * rot_quat.z), 2.0 * (rot_quat.y * rot_quat.z - rot_quat.w * rot_quat.x)),\n        vec3(2.0 * (rot_quat.x * rot_quat.z - rot_quat.w * rot_quat.y), 2.0 * (rot_quat.y * rot_quat.z + rot_quat.w * rot_quat.x), 1.0 - 2.0 * (rot_quat.x * rot_quat.x + rot_quat.y * rot_quat.y))\n    );\n    hdrPixel = mat3(rot_Matrix) * hdrPixel;\n\n    hdrPixel *= EnvironmentScale;\n    return hdrPixel;\n}\n\nvec2 Hammersley(uint i, uint N) {\n    uint bits = (i \u003c\u003c 16u) | (i \u003e\u003e 16u);\n    bits = ((bits \u0026 0x55555555u) \u003c\u003c 1u) | ((bits \u0026 0xAAAAAAAAu) \u003e\u003e 1u);\n    bits = ((bits \u0026 0x33333333u) \u003c\u003c 2u) | ((bits \u0026 0xCCCCCCCCu) \u003e\u003e 2u);\n    bits = ((bits \u0026 0x0F0F0F0Fu) \u003c\u003c 4u) | ((bits \u0026 0xF0F0F0F0u) \u003e\u003e 4u);\n    bits = ((bits \u0026 0x00FF00FFu) \u003c\u003c 8u) | ((bits \u0026 0xFF00FF00u) \u003e\u003e 8u);\n    float rdi = float(bits) * 2.3283064365386963e-10;\n    return vec2(float(i) / float(N), rdi);\n}\n\nvec3 importanceSampleGGX(vec2 Xi, float roughness, vec3 N) {\n    float a = roughness * roughness;\n    float Phi = 2.0 * PI * Xi.x;\n    float CosTheta = sqrt((1.0 - Xi.y) / (1.0 + (a * a - 1.0) * Xi.y));\n    float SinTheta = sqrt(1.0 - CosTheta * CosTheta);\n    vec3 H;\n    H.x = SinTheta * cos(Phi);\n    H.y = SinTheta * sin(Phi);\n    H.z = CosTheta;\n    vec3 UpVector = abs(N.z) \u003c 0.999 ? vec3(0, 0, 1) : vec3(1, 0, 0);\n    vec3 TangentX = normalize(cross(UpVector, N));\n    vec3 TangentY = cross(N, TangentX);\n    return TangentX * H.x + TangentY * H.y + N * H.z;\n}\n\nfloat specularD(float roughness, float NoH) {\n    float a2 = roughness * roughness;\n    float d = (NoH * a2 - NoH) * NoH + 1.0;\n    return a2 / (PI * d * d);\n}\n\nvec4 sumSpecular(vec3 spec, float NoL, vec4 result) {\n    result.xyz += spec;\n    result.w += NoL;\n    return result;\n}\n\nvec3 ImportanceSample(vec3 R) {\n    vec3 N = normalize(R);\n    vec3 V = normalize(R);\n    vec4 result = vec4(0.0);\n\n    float sampleStep = float(ConvolutionMaxSamples) / float(ConvolutionSampleCount);\n    uint sampleId = uint(ConvolutionSamplesOffset);\n\n    int cubeWidth = textureSize(envCubemap, 0).x;\n\n    for (int i = 0; i \u003c ConvolutionSampleCount; i++) {\n        vec2 Xi = Hammersley(sampleId, uint(ConvolutionMaxSamples));\n\n        vec3 H = importanceSampleGGX(Xi, ConvolutionRoughness, N);\n        vec3 L = 2.0 * dot(V, H) * H - V;\n        float NoL = max(dot(N, L), 0.0);\n        float VoL = max(dot(V, L), 0.0);\n        float NoH = max(dot(N, H), 0.0);\n        float VoH = max(dot(V, H), 0.0);\n        if (NoL \u003e 0.0) {\n            float Dh = specularD(ConvolutionRoughness, NoH);\n            float pdf = Dh * NoH / (4.0 * VoH);\n            float solidAngleTexel = 4.0 * PI / (6.0 * cubeWidth * cubeWidth);\n            float solidAngleSample = 1.0 / (ConvolutionSampleCount * pdf);\n            float lod = ConvolutionRoughness == 0.0 ? 0.0 : 0.5 * log2(solidAngleSample / solidAngleTexel);\n\n            vec3 hdrPixel = rescaleHDR(textureLod(envCubemap, L, lod).rgb);\n            // vec3 hdrPixel = textureLod(envCubemap, L, lod).rgb;\n            result = sumSpecular(hdrPixel, NoL, result);\n        }\n        sampleId += uint(sampleStep);\n    }\n\n    if (result.w == 0.0)\n        return result.xyz;\n    else\n        return result.xyz / result.w;\n}\n\nvoid main() {\n    vec3 N = normalize(WorldPos);\n    vec3 color;\n\n    vec3 importanceSampled = ImportanceSample(N);\n\n    if (ConvolutionSamplesOffset \u003e= 1) {\n        vec3 lastResult = textureLod(envCubemap, N, ConvolutionMip).rgb;\n        color = mix(lastResult, importanceSampled, 1.0 / float(ConvolutionSamplesOffset));\n    } else {\n        color = importanceSampled;\n    }\n\n    FragColor = vec4(color, 1.0);\n}\n```\n\n### Specular Map\n\nI decided to just use the environment map as it is. This is because the results from directly applying pre-filtered environment maps to mipmaps, as demonstrated in LearnOpenGL, didn't show significant differences compared to the mipmaps provided by OpenGL.\n\nWe are going to implement specular using an environment map, a BRDF texture, and mipmaps.\n\n### BRDF Texture\n\nThe BRDF (Bidirectional Reflectance Distribution Function) texture is used to define the surface properties of an object. This texture is composed of a function that represents how light is reflected from the surface of the object, indicating the direction and amount of reflection. In other words, it determines how light coming from a certain direction is reflected in various directions from the surface. The BRDF texture is used to define properties such as surface roughness, glossiness, and reflection characteristics.\n\nIn environment lighting, the BRDF texture is primarily used to model the interaction of light reflection on the surface of an object. This texture determines how light is reflected from the surface of the object depending on its properties when the light reaches the surface, thus making the appearance of the object more realistic.\n\nFor example, when the surface of an object is rough and lacks glossiness, the BRDF texture can accurately model how light is reflected from the surface of the object. This allows for a more accurate simulation of how light generated from environment lighting is reflected from the surface of the object.\n\nHere, the code for generating the BRDF texture from the Environment Map was directly borrowed from [Learn OpenGL](https://learnopengl.com/).\n\n```glsl\n#version 420 core\nout vec2 FragColor;\nin vec2 TexCoords;\n\nconst float PI = 3.14159265359;\n\n// http://holger.dammertz.org/stuff/notes_HammersleyOnHemisphere.html\n// efficient VanDerCorpus calculation.\nfloat RadicalInverse_VdC(uint bits)\n{\n     bits = (bits \u003c\u003c 16u) | (bits \u003e\u003e 16u);\n     bits = ((bits \u0026 0x55555555u) \u003c\u003c 1u) | ((bits \u0026 0xAAAAAAAAu) \u003e\u003e 1u);\n     bits = ((bits \u0026 0x33333333u) \u003c\u003c 2u) | ((bits \u0026 0xCCCCCCCCu) \u003e\u003e 2u);\n     bits = ((bits \u0026 0x0F0F0F0Fu) \u003c\u003c 4u) | ((bits \u0026 0xF0F0F0F0u) \u003e\u003e 4u);\n     bits = ((bits \u0026 0x00FF00FFu) \u003c\u003c 8u) | ((bits \u0026 0xFF00FF00u) \u003e\u003e 8u);\n     return float(bits) * 2.3283064365386963e-10; // / 0x100000000\n}\n\nvec2 Hammersley(uint i, uint N)\n{\n\treturn vec2(float(i)/float(N), RadicalInverse_VdC(i));\n}\n\nvec3 ImportanceSampleGGX(vec2 Xi, vec3 N, float roughness)\n{\n\tfloat a = roughness*roughness;\n\n\tfloat phi = 2.0 * PI * Xi.x;\n\tfloat cosTheta = sqrt((1.0 - Xi.y) / (1.0 + (a*a - 1.0) * Xi.y));\n\tfloat sinTheta = sqrt(1.0 - cosTheta*cosTheta);\n\n\t// from spherical coordinates to cartesian coordinates - halfway vector\n\tvec3 H;\n\tH.x = cos(phi) * sinTheta;\n\tH.y = sin(phi) * sinTheta;\n\tH.z = cosTheta;\n\n\t// from tangent-space H vector to world-space sample vector\n\tvec3 up          = abs(N.z) \u003c 0.999 ? vec3(0.0, 0.0, 1.0) : vec3(1.0, 0.0, 0.0);\n\tvec3 tangent   = normalize(cross(up, N));\n\tvec3 bitangent = cross(N, tangent);\n\n\tvec3 sampleVec = tangent * H.x + bitangent * H.y + N * H.z;\n\treturn normalize(sampleVec);\n}\n\nfloat GeometrySchlickGGX(float NdotV, float roughness)\n{\n    // note that we use a different k for IBL\n    float a = roughness;\n    float k = (a * a) / 2.0;\n\n    float nom   = NdotV;\n    float denom = NdotV * (1.0 - k) + k;\n\n    return nom / denom;\n}\n\nfloat GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)\n{\n    float NdotV = max(dot(N, V), 0.0);\n    float NdotL = max(dot(N, L), 0.0);\n    float ggx2 = GeometrySchlickGGX(NdotV, roughness);\n    float ggx1 = GeometrySchlickGGX(NdotL, roughness);\n\n    return ggx1 * ggx2;\n}\n\nvec2 IntegrateBRDF(float NdotV, float roughness)\n{\n    vec3 V;\n    V.x = sqrt(1.0 - NdotV*NdotV);\n    V.y = 0.0;\n    V.z = NdotV;\n\n    float A = 0.0;\n    float B = 0.0;\n\n    vec3 N = vec3(0.0, 0.0, 1.0);\n\n    const uint SAMPLE_COUNT = 1024u;\n    for(uint i = 0u; i \u003c SAMPLE_COUNT; ++i)\n    {\n        // generates a sample vector that's biased towards the\n        // preferred alignment direction (importance sampling).\n        vec2 Xi = Hammersley(i, SAMPLE_COUNT);\n        vec3 H = ImportanceSampleGGX(Xi, N, roughness);\n        vec3 L = normalize(2.0 * dot(V, H) * H - V);\n\n        float NdotL = max(L.z, 0.0);\n        float NdotH = max(H.z, 0.0);\n        float VdotH = max(dot(V, H), 0.0);\n\n        if(NdotL \u003e 0.0)\n        {\n            float G = GeometrySmith(N, V, L, roughness);\n            float G_Vis = (G * VdotH) / (NdotH * NdotV);\n            float Fc = pow(1.0 - VdotH, 5.0);\n\n            A += (1.0 - Fc) * G_Vis;\n            B += Fc * G_Vis;\n        }\n    }\n    A /= float(SAMPLE_COUNT);\n    B /= float(SAMPLE_COUNT);\n    return vec2(A, B);\n}\n\nvoid main()\n{\n    vec2 integratedBRDF = IntegrateBRDF(TexCoords.x, TexCoords.y);\n    FragColor = integratedBRDF;\n}\n```\n\n### Ambient Lighting\n\nNow we calculate indirect lighting using the given textures. Here, we use Image-Based Lighting (IBL) for this purpose.\n\nFirst, we compute the Fresnel reflection coefficient on the surface. This coefficient determines the ratio of incident light to reflected light and is used in conjunction with the surface roughness.\n\n```glsl\nvec3 fresnelSchlickRoughness(float cosTheta, vec3 F0, float roughness)\n{\n    return F0 + (max(vec3(1.0 - roughness), F0) - F0) * pow(clamp(1.0 - cosTheta, 0.0, 1.0), 5.0);\n}\n\n// ambient lighting (we now use IBL as the ambient term)\nvec3 F = fresnelSchlickRoughness(max(dot(N, V), 0.0), F0, roughness);\n```\n\nBased on the Fresnel coefficient, we calculate the Specular and Diffuse coefficients. kS represents the Specular coefficient, while KD represents the Diffuse coefficient. These coefficients are determined based on the surface's metallic property.\n\n```glsl\nvec3 kS = F;\nvec3 kD = 1.0 - kS;\nkD \\*= 1.0 - metallic;\n```\n\nWe calculate the surface's environmental lighting using the Irradiance Cubemap. By sampling the Irradiance Cubemap with the surface's normal vector (N), we obtain the diffuse component by multiplying the result with the surface's albedo.\n\n```glsl\nvec3 irradiance = texture(irradianceCubemap, N).rgb;\nvec3 diffuse = irradiance \\* albedo;\n```\n\nWe compute the specular component using the Pre-filtered Environment Map and the BRDF Look-Up Table (LUT). The Pre-filtered Environment Map represents the level of reflected environment map details based on the surface's roughness. The BRDF LUT is a precomputed table of BRDF values based on the angle and roughness between the surface's normal vector and the view vector. By combining these, we obtain the specular component.\n\n```glsl\n// sample both the pre-filter map and the BRDF lut and combine them together as per the Split-Sum approximation to get the IBL specular part.\nconst float MAX_REFLECTION_LOD = 4.0;\nvec3 prefilteredColor = textureLod(preFilteredEnvmap, R, roughness _ MAX_REFLECTION_LOD).rgb;\nvec2 brdf = texture(brdfLUT, vec2(max(dot(N, V), 0.0), roughness)).rg;\nspecular = prefilteredColor * (F \\* brdf.x + brdf.y);\n```\n\nWe combine the diffuse and specular components to compute the overall indirect lighting. We multiply this by the Ambient Occlusion (AO) value to obtain the final indirect lighting.\n\n```glsl\nvec3 ambient = (kD * diffuse + specular) * ao;\n```\n\n---\n\n## 4. Post Processing\n\n### HDR Tonemapping\n\nHDR images represent a wide range of luminance values, requiring proper tonemapping to display them accurately on standard display devices. In this code, Reinhard tonemapping is used to compress the brightness and bring the dynamic range of the image into a suitable range.\n\nReinhard tonemapping is a method used to compress the dynamic range of high dynamic range (HDR) images so that they can be accurately displayed on low dynamic range (LDR) devices, such as computer monitors and TVs. This technique adjusts the brightness of the pixels in the HDR image based on their luminance values, aiming to maintain natural-looking contrast while preventing overexposure or loss of detail in both shadow and highlight areas.\n\n### Gamma Correction\n\nHDR operations are performed entirely in linear space. In linear space, adjustments to brightness and color occur linearly, enabling more accurate and predictable calculations. However, most screens and image storage media use gamma-corrected nonlinear spaces. This helps efficiently represent details in bright areas by considering human visual characteristics.\n\nTherefore, before displaying HDR images, they need to be transformed from linear space to nonlinear space through gamma correction. This preserves details in bright areas better and results in a more natural display on the screen.\n\n```glsl\nvec3 color = directLight + ambient;\n\n// HDR tonemapping\ncolor = color / (color + vec3(1.0));\n\n// gamma correct\ncolor = pow(color, vec3(1.0/2.2));\n```\n\n---\n\n## 5. WebGL\n\nI've migrated the code implemented in OpenGL to WebGL. However, dynamically implementing IBL textures like in the native environment was cumbersome, and there were difficulties in loading HDRI textures. So, I decided to first create a native texture generation app called \"IBL Maker\" to generate textures in advance. Then, I used those textures in WebGL to implement PBR.\n\nSince I saved the textures in JPG format, I had to pay attention to gamma correction when saving textures in OpenGL and loading textures in WebGL, unlike in the native environment.\n\nIn the native code, tonemapping and gamma correction were done as follows:\n\n```glsl\nvec3 color = ...;\n\n// HDR tonemapping\ncolor = color / (color + vec3(1.0));\n\n// prevent clipping\ncolor = min(color, vec3(1.0));\n\n// gamma correct\ncolor = pow(color, vec3(1.0/2.2));\n```\n\nIn WebGL, I performed gamma correction each time a texture was loaded to linearize it:\n\n```glsl\nvec3 envMap = pow(textureLod(envCubemap, R, roughness * MAX_REFLECTION_LOD).rgb, vec3(2.2));\n\n```\n\n### P.S.\n\nWhile learning the WebGPU API, I rewrote this example using WebGPU. You can check out the process at the link below.\n\n[Introduce to WebGPU](/post/immutability_object_freeze_doesnt_always_freeze)\n\n### References\n\n- [Learn OpenGL](https://learnopengl.com/)\n- [Normal Transformation](https://paroj.github.io/gltut/Illumination/Tut09%20Normal%20Transformation.html)\n- [SIGGRAPH 2013 Course: Physically Based Shading in Theory and Practice](https://blog.selfshadow.com/publications/s2013-shading-course)\n- [IBL Baker](https://www.derkreature.com/iblbaker)\n- [Tonemapping - 64.github.io](https://64.github.io/tonemapping/)\n- [Introduction to Computer Graphics with DirectX 11 - Part 3. Rendering Techniques](https://honglab.co.kr/courses/graphicspt3)\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/2e2a7597edf4d608.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"RE4Mo7A9A9aWm64uQv2d1\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/post/pbr-rendering/\",\"initialTree\":[\"\",{\"children\":[\"post\",{\"children\":[[\"slug\",\"pbr-rendering\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"pbr-rendering\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L5\"],\"globalErrorComponent\":\"$6\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"mx-auto my-0\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"max-w-default mx-auto px-5 py-3\",\"children\":[\"$\",\"$L7\",null,{}]}],[\"$\",\"header\",null,{\"children\":[\"$\",\"$L8\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"max-w-default mx-auto p-5\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"post\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[null,[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"post\",\"children\",[\"slug\",\"pbr-rendering\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lb\",[\"$\",\"div\",null,{\"children\":[[\"$\",\"$Lc\",null,{\"href\":\"/posts/graphics\",\"className\":\"text-gray-600 text-md pl-1 hover:font-bold\",\"children\":\"GRAPHICS\"}],[\"$\",\"h1\",null,{\"children\":\"PBR Rendering\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-400 text-sm m-0 mt-1\",\"children\":\"1 April, 2024\"}],[\"$\",\"div\",null,{\"className\":\"mt-1\",\"children\":[\"$\",\"div\",null,{\"className\":\"whitespace-nowrap overflow-x-auto pb-3\",\"children\":[[\"$\",\"$Lc\",\"opengl\",{\"href\":\"/tag/opengl\",\"className\":\"mr-2 text-gray-500 hover:font-bold\",\"children\":\"#opengl\"}],[\"$\",\"$Lc\",\"pbr\",{\"href\":\"/tag/pbr\",\"className\":\"mr-2 text-gray-500 hover:font-bold\",\"children\":\"#pbr\"}],[\"$\",\"$Lc\",\"glsl\",{\"href\":\"/tag/glsl\",\"className\":\"mr-2 text-gray-500 hover:font-bold\",\"children\":\"#glsl\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"mt-8\",\"children\":[\"$\",\"$Ld\",null,{\"content\":\"$e\"}]}],[\"$\",\"$Lf\",null,{\"issueTerm\":\"PBR Rendering\"}]]}],null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"pbr-rendering\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/602feae4669a507e.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}],null],\"segment\":[\"slug\",\"pbr-rendering\",\"d\"]},\"styles\":[]}],\"segment\":\"post\"},\"styles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"h-40 flex justify-center items-end\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-500 text-sm\",\"children\":\"(C) 2021. Wayne Choi. All rights reserved.\"}]}]]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"WayneChoi.Dev\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Wayne Choi blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}]]\nb:null\n"])</script></body></html>